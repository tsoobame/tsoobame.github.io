{"pageProps":{"posts":[{"title":"Understanding data loader","date":"2020-07-19","description":"Create a basic dataloader to understand this great library","thumbnail":{"url":"/images/data-loader.jpeg","attribution":{"name":"Lars Kienle","url":"https://unsplash.com/@larskienle?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"}},"tags":["javascript"],"filePath":"data-loader.mdx","slug":"data-loader","markdown":"# Understanding the Dataloader\n\nDataloader is one of the packages I find more useful and smart from the ones I have in my toolbox.\n\nI am going to set up a obvious naive example and follow the process to build a simple dataloader to understand its beauty and how useful it is.\n\n## About the project\n\nWe are going to create a view and api over a social network. Our users relations are:\n\n```text\nUser 1 friend of [ 2, 3 ]\nUser 2 friend of [ 1, 3 ]\nUser 3 friend of [ 1, 2, 4 ]\nUser 4 friend of [ 3, 5 ]\nUser 5 friend of [ 4 ]\n```\n\nThe view can show the relation between users and their friends. We can show N levels of their friendship. We are not goint to look much at it in this post.\n\nUsers data can be found here.\n\nThe only dependency will be express.\n\n## Initial Setup\n\n### datasource.js\n\nThe datasource allows us to retrieve one or multiple users by id. Contract is not random, it is already based on the real dataloader so there will be minimal changes over the course of the post. Data is defined in a file within the project. Code is pretty simple:\n\n```javascript\nconst users = require(\"./users.json\");\n\nconst getUsersFromFile = (ids) =>\n  ids.map((id) => users.find((u) => u.id === id));\n\nconst sleep = (ms) => new Promise((resolve) => setTimeout(resolve, ms));\n\nasync function loadMany(ids) {\n  console.log(`GET /users?ids=${ids}`);\n\n  await sleep(100);\n  return getUsersFromFile(ids);\n}\n\nasync function load(id) {\n  const results = await loadMany([id]);\n  return results[0];\n}\n\nmodule.exports = {\n  load,\n  loadMany,\n};\n```\n\nThe only interesting method is loadMany. We will print the requests to the simulated service so we can check the console. There will be a delay to resolve the promise, so we can simulate better and understand why dataloader is so good.\n\nA very important requirement is that data needs to be returned to the caller in the right order and all elements need to be returned (same length of ids and results arrays). This will be clear when we put in place the dataloader.\n\n### resolver.js\n\nResolver will use the datasource received by parameter to load friendship data about users. It can receive the levels of friends we want to get, so it will use a recursive approach to load friends of friends until all levels are fetched.\n\n```javascript\nasync function getFriends(datasource, user, levels) {\n  if (levels == 0) {\n    return { id: user.id, name: user.name };\n  }\n\n  const friends = await datasource.loadMany(user.friends);\n\n  return {\n    ...user,\n    friends: await Promise.all(\n      friends.map((f) => getFriends(datasource, f, levels - 1))\n    ),\n  };\n}\n\nasync function getUserWithFriends(datasource, id, levels = 1) {\n  const user = await datasource.load(id);\n  return getFriends(datasource, user, levels);\n}\n\nmodule.exports = { getUserWithFriends };\n```\n\nIt uses a brute force approach on purpose. The code is simple but far away from being optimal. In one method it looks obvious, but sometimes, when we are building graphql or similar apis, or complex workflows we might be doing exactly this kind of brute force requests.\n\n### view.js\n\nNothing advanced. Just render users friends in a nested way.\n\n```javascript\nfunction render(user) {\n  return `<div style=\"padding-left: 12px;background-color:#def\"> ${user.name} ${\n    user.friends ? user.friends.map((u) => render(u)).join(\"\") : \"\"\n  } </div>`;\n}\n\nmodule.exports = {\n  render,\n};\n```\n\n### server.js\n\n```javascript\nconst express = require(\"express\");\nconst PORT = 3000;\nconst app = express();\n\nconst datasource = require(\"./datasource\");\nconst resolver = require(\"./resolver\");\nconst view = require(\"./view\");\n\napp.get(`/user-with-friends/:id`, async (req, res) => {\n  const id = req.params.id;\n  const levels = req.query.levels || 1;\n\n  const user = await resolver.getUserWithFriends(datasource, id, levels);\n\n  res.send(view.render(user));\n});\n\napp.listen(PORT, () => console.log(`Fakebook listening to ${PORT}`));\n```\n\n## Run\n\n```shell\nnode index.js\n```\n\n## Test 1\n\nWe will render friends of user 1. Only 1 level:\n\n```text\nhttp://localhost:3000/user-with-friends/1\n```\n\nIf we check in our console we will find:\n\n```text\nGET /users?ids=1\nGET /users?ids=2,3\n```\n\nAll good. We requested user 1 and their friends 2 and 3.\n\n## Test 2\n\nLet's try by loading 3 levels:\n\n```text\nhttp://localhost:3000/user-with-friends/1?levels=3\n```\n\nThings are getting interesting here:\n\n```text\nGET /users?ids=1\nGET /users?ids=2,3\nGET /users?ids=1,3\nGET /users?ids=1,2,4\nGET /users?ids=2,3\nGET /users?ids=1,2,4\nGET /users?ids=2,3\nGET /users?ids=1,3\nGET /users?ids=3,5\n```\n\nWe are loading data for users 1,2,3,4,5 but we are doing 9 requests. We are requesting the same users again and again. We could easily improve the situation adding some sort of cache per request.\n\nCache per request\nWe are going to add a cache to the system. It will be empty at the start of each request, so we do not need to worry about expirations. The benefits will be:\n\nDo not request the same resource twice to the remote source during the same request.\nAs side effect, if we try to get the same resource twice during the same request, we will get the same data. So mutations of the resources in between a request will not provide incoherent results.\n\n### cache.js\n\nSimple cache implementation:\n\n```javascript\nfunction make(loadManyFn) {\n  const cache = {};\n\n  async function loadMany(ids) {\n    const notCachedIds = ids.filter((id) => !cache[id]);\n\n    if (notCachedIds.length > 0) {\n      const results = await loadManyFn(notCachedIds);\n      notCachedIds.forEach((id, idx) => (cache[id] = results[idx]));\n    }\n\n    return ids.map((id) => cache[id]);\n  }\n\n  return {\n    load: async (id) => {\n      const results = await loadMany([id]);\n      return results[0];\n    },\n    loadMany,\n  };\n}\n\nmodule.exports = { make };\n```\n\nCache needs a function to retrieve multiple data by id (or in general by a key). It will check the data that is cached and request only the ids that are not found.\n\nImplements the same contract as datasource.\n\n### server.js\n\nLet's add this line to the server:\n\n```javascript\nconst cache = require('./cache')\nAnd replace this line:\n\nconst user = await resolver.getUserWithFriends(datasource, id, levels)\nwith:\n\nconst user = await resolver.getUserWithFriends(cache.make(datasource.loadMany), id, levels)\n```\n\n## Run\n\nLet's run again the server and test the previous request:\n\n```text\nhttp://localhost:3000/user-with-friends/1?levels=3\n```\n\n```text\nGET /users?ids=1\nGET /users?ids=2,3\nGET /users?ids=4\nGET /users?ids=4\nGET /users?ids=5\n```\n\nWe could reduce the number of requests from 9 to 5, which is pretty good. But, what a momentwhat happened here? Why are we requesting id=4 twice?\n\nIf we unnest the request flow based on how nodejs works (and how we implemented our resolver) this is what happened:\n\n```text\n1 - Load user 1 => GET /users?ids=1\n2 - Load friends of 1: [2,3]=> GET /users?ids=2,3\n3.1. Load friends of 2: [1,3] => all cached\n4.1. Load friends of 1 : [2,3] => all cached\n4.2. Load friends of 3 : [1,2,4] => GET /users?ids=4\n3.2. Load friends of 3: [1,2,4] => GET /users?ids=4\n4.3. Load friends of 1: [2,3] => all cached\n4.4. Load friends of 2: [1,3] => all cached\n4.5. Load friends of 4: [3,5] => GET /users?ids=5\nOn 3.1 we had all friends of user 2 cached. So the code was straight to 4.2, than ran in parallel with 3.2. Both were waiting for the same user (4) and therefore made the same requests twice.\n```\n\nSo with our simple cache, we did not reduce the requests to the minimun we wanted.\n\nFor example, if we did:\n\n```javascript\nconst users = await Promise.all(load(1), load(1));\n```\n\nThere would be 2 requests before the cache has data for id=1.\n\nLet's fix this and produce the ideal:\n\n```text\nGET /users?ids=1\nGET /users?ids=2,3\nGET /users?ids=4\nGET /users?ids=5\n```\n\n## Dataloader\n\nUsing nodejs `process.nextTick(...)` we can postpone the execution of a given function to the end of the current event loop cycle. It is useful to run a given function after all variables are initialized for example.\n\nFrom nodejs documentation:\n\n```text\nBy using process.nextTick() we guarantee that apiCall() always runs its callback after the rest of the user's code and before the event loop is allowed to proceed.\n```\n\nUsing it we can accumulate all the keys that are being requested during the same cycle (3.2 and 4.2 in the example above) and request them at the end. In the next cycle we would accumulate again the ones that were depending in the previous ones and so on.\n\nThis simple version of dataloader incorporates also code to accomplish the cache:\n\n```javascript\nfunction make(loadManyFn) {\n  const cache = {};\n  let pending = [];\n  let scheduled = false;\n  function scheduleSearch() {\n    if (pending.length > 0 && !scheduled) {\n      scheduled = true;\n      Promise.resolve().then(() =>\n        process.nextTick(async () => {\n          await runSearch();\n          scheduled = false;\n        })\n      );\n    }\n  }\n\n  async function runSearch() {\n    const pendingCopy = pending.splice(0, pending.length);\n    pending = [];\n\n    if (pendingCopy.length > 0) {\n      const results = await loadManyFn(pendingCopy.map((p) => p.id));\n      pendingCopy.forEach(({ resolve }, idx) => resolve(results[idx]));\n    }\n  }\n\n  async function loadMany(ids) {\n    const notCachedIds = ids.filter((id) => !cache[id]);\n\n    if (notCachedIds.length > 0) {\n      notCachedIds.map((id) => {\n        cache[id] = new Promise((resolve) => {\n          pending.push({ id, resolve });\n        });\n      });\n\n      scheduleSearch();\n    }\n\n    return Promise.all(ids.map((id) => cache[id]));\n  }\n\n  return {\n    load: async (id) => {\n      const results = await loadMany([id]);\n      return results[0];\n    },\n    loadMany,\n  };\n}\n\nmodule.exports = { make };\n```\n\nIgnoring the part of the cache, the important bits are:\n\n### Accumulating requests\n\n```javascript\nnotCachedIds.map((id) => {\n  cache[id] = new Promise((resolve) => {\n    pending.push({ id, resolve });\n  });\n});\n```\n\nWe will add to the list of pending ids the ones that are not cached. We will keep the id and the resolve method, so we can resolve them afterwards with the right value. We cache the promise itself in the hashmap. This would allow us to cache also rejected promises for example. So we do not request over and over the same rejection. It is not used in this implementation, though.\n\n### Scheduling the request\n\n```javascript\nfunction scheduleSearch() {\n  if (pending.length > 0 && !scheduled) {\n    scheduled = true;\n    Promise.resolve().then(() =>\n      process.nextTick(async () => {\n        await runSearch();\n        scheduled = false;\n      })\n    );\n  }\n}\n```\n\nThat is where the magic happens. This function is short but is the most important one: We schedule/delay the request to the end of all the promises declarations.\n\n### Executing the search\n\n```javascript\nasync function runSearch() {\n  const pendingCopy = pending.splice(0, pending.length);\n  pending = [];\n\n  if (pendingCopy.length > 0) {\n    const results = await loadManyFn(pendingCopy.map((p) => p.id));\n    pendingCopy.forEach(({ resolve }, idx) => resolve(results[idx]));\n  }\n}\n```\n\nClone the ids (so they can be accumulated again after the search completes) and call the loadManyFn so we can resolve the promises we had pending. Remember the requirements of loadMany to return the data in the right order and all the elements ? This is where it is needed. We can reference the results by index and resolve the right pending promises.\n\nLet's run it!\n\n## Execution\n\nAgain the same request:\n\n```text\nhttp://localhost:3000/user-with-friends/1?levels=3\n```\n\nThat produces the following output:\n\n```text\nGET /users?ids=1\nGET /users?ids=2,3\nGET /users?ids=4\nGET /users?ids=5\n```\n\nExactly what we wanted.\n\n## Conclusion\n\n- Dataloader is a great package that should be in all developers toolbox. Specially the ones implementing Graphql or similar Apis.\n\n- The resolvers in this example could be optimized but sometimes our requests are on different files at different levels that depend on some conditions. With Dataloader we can keep our file structure and code readability without damaging our performance, both on response time to our client and on number of requests spawn within our mesh.\n\nAre you using Dataloader? Do you know any tool that accomplishes something similar? Do you now any other packages that in your opinion should be in all nodejs devs toolbox?\n"},{"title":"Graphql Schema Stitching","date":"2019-04-26","description":"What Graphql schema stitching is and how it can help us","thumbnail":{"url":"/images/graphql-schema-stitching.jpeg","attribution":{"name":"Aneta Pawlik","url":"https://unsplash.com/@anetakpawlik?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"}},"tags":["javascript","graphql","api"],"filePath":"graphql-schema-stitching.mdx","slug":"graphql-schema-stitching","markdown":"# Graphql Schema Stitching\n\nI am going to write a short (?) post about how to create a simple API Gateway that exposes two services using Graphql Stitching. I am assuming some knowledge about graphql and Apollo Server.\nWe will use express, nodejs and apollo for the service and a technique called schema stitching.\nIf you want to learn more about Graphql you can go to the official site.\n\n## Why do we need Api gateways and schema stitching\n\nI will write a whole post about the reasons we had to use Graphql in our services and in our Api Gateway.\nHere I am offering a short explanation:\nIn real world scenarios we are creating independent and autonomous (micro)services. The less data they share, the less they need to call each other and the less coupled they are, the better.\nMany times a service manages entities (or parts of entities) that hold an id about another entity but does not need to know more details. For example an inventory service might manage productID and available units, but does not need to know about the name of the product or about its price.\nInventory service will be able to run all its operations and apply the rules it manages without requesting information to any other service.\nUsers, on the other hand, will need to see this scattered data together in one screen. In order to avoid too many requests from the UI, an API Gateway can offer a single endpoint where UI can request the data needed for a specific functionality/screen in one request, and the Gateway can orchestrate the calls to other services, cache results if needed, etc.\n\n## Let&#x27;s start working\n\nLet&#x27;s create a folder as the root for our project:\n\n```shell\nmkdir graphql-stitching\ncd graphql-stitching\n```\n\n## Creating the songs service\n\nWe are going to create a simple service that offers data about songs.\n\n```shell\nmkdir songs\ncd songs\nnpm init -y\nnpm install express graphql apollo-server-express body-parser\n```\n\nWe are going to create our schema first:\n\n```shell\ntouch schema.js\n```\n\n#### schema.js\n\n```javascript\nconst { makeExecutableSchema } = require(\"graphql-tools\");\nconst gql = require(\"graphql-tag\");\n\nconst songs = [\n  { id: 1, title: \"I will always love you\" },\n  { id: 2, title: \"Lose yourself\" },\n  { id: 3, title: \"Eye of the tiger\" },\n  { id: 4, title: \"Men in Black\" },\n  { id: 5, title: \"The power of love\" },\n  { id: 6, title: \"My Heart will go on\" },\n];\n\nconst typeDefs = gql`\n  type Query {\n    songs: [Song]\n    song(songId: ID!): Song\n  }\n  type Song {\n    id: ID\n    title: String\n  }\n`;\n\nconst resolvers = {\n  Query: {\n    songs: () => {\n      return songs;\n    },\n    song(parent, args, context, info) {\n      return songs.find((song) => song.id === Number(args.songId));\n    },\n  },\n};\n\nmodule.exports = makeExecutableSchema({\n  typeDefs,\n  resolvers,\n});\n```\n\nWe are defining a list of songs.\nThe type Song (id, title) and two queries for getting all songs and one song by id.\n\nLet&#x27;s create the api:\n\n```shell\ntouch index.js\n```\n\n### index.js\n\n```javascript\nconst express = require(\"express\");\nconst { ApolloServer } = require(\"apollo-server-express\");\nconst cors = require(\"cors\");\nconst schema = require(\"./schema\");\nconst bodyParser = require(\"body-parser\");\n\nconst app = express();\napp.use(cors());\napp.use(bodyParser.json());\n\nconst server = new ApolloServer({\n  playground: {\n    endpoint: \"/api\",\n    settings: {\n      \"editor.cursorShape\": \"block\",\n      \"editor.cursorColor\": \"#000\",\n      \"editor.theme\": \"light\",\n    },\n  },\n  schema,\n});\n\nserver.applyMiddleware({ app, path: \"/api\" });\n\napp.listen(3000, () => {\n  console.log(\"Song services listening to 3000...\");\n});\n```\n\nWe create a simple express service using apollo engine to expose both the api and the playground to tests our api.\n\n```shell\nnode index.js\n```\n\nand open the <a href=\"http://localhost:3000\">songs api</a>\nYou will see the playground, so you can run the first query:\n\n```graphql\n{\n  songs {\n    id\n    title\n  }\n}\n```\n\nyou should be able to see the results.\n\n## Creating the movies service\n\nWe are going to follow the same process. From the root of our project:\n\n```shell\nmkdir movies\ncd movies\ntouch index.js\ntouch schema.js\nnpm init -y\nnpm install express graphql apollo-server-express body-parser graphql-tag\n```\n\nindex.js will be similar to the previous one. Only the port number needs to be different\n\n```javascript\nconst express = require(\"express\");\nconst { ApolloServer } = require(\"apollo-server-express\");\nconst cors = require(\"cors\");\nconst schema = require(\"./schema\");\nconst bodyParser = require(\"body-parser\");\n\nconst app = express();\napp.use(cors());\napp.use(bodyParser.json());\n\nconst server = new ApolloServer({\n  playground: {\n    endpoint: \"/api\",\n    settings: {\n      \"editor.cursorShape\": \"block\",\n      \"editor.cursorColor\": \"#000\",\n      \"editor.theme\": \"light\",\n    },\n  },\n  schema,\n});\n\nserver.applyMiddleware({ app, path: \"/api\" });\n\napp.listen(3001, () => {\n  console.log(\"Movie services listening to 3001...\");\n});\n```\n\nSchema will be very similar:\n\n```javascript\nconst { makeExecutableSchema } = require(\"graphql-tools\");\nconst gql = require(\"graphql-tag\");\n\nconst movies = [\n  { id: 1, title: \"The Bodyguard\", mainSongId: 1 },\n  { id: 2, title: \"8 Mile\", mainSongId: 2 },\n  { id: 3, title: \"Rocky III\", mainSongId: 3 },\n  { id: 4, title: \"Men in Black\", mainSongId: 4 },\n  { id: 5, title: \"Back to the Future\", mainSongId: 5 },\n  { id: 6, title: \"Titanic\", mainSongId: 6 },\n];\n\nconst typeDefs = gql`\n  type Query {\n    movies: [Movie]\n    movie(movieId: ID!): Movie\n  }\n  type Movie {\n    id: ID!\n    title: String!\n    mainSongId: ID!\n  }\n`;\n\nconst resolvers = {\n  Query: {\n    movies: () => {\n      return movies;\n    },\n    movie(parent, args, context, info) {\n      return movies.find((movie) => movie.id === Number(args.movieId));\n    },\n  },\n};\n\nmodule.exports = makeExecutableSchema({\n  typeDefs,\n  resolvers,\n});\n```\n\nThe difference is that movie has a reference to songs. Specifically mainSongId. Since both services are isolated and are autonomous, movie service does not know where songs service is, or what data a songs holds. Only knows that a movie has a main song and it holds its ID.\n\nIf we run the project in the same way\n\n```shell\nnode index.js\n```\n\nwe can see the <a href=\"http://localhost:3001\">playground</a> and run our test queries.\n\n## Let&#x27;s start the interesting part. Our Api gateway\n\nWe are going to create the same files. From project root:\n\n```shell\nmkdir apigateway\ncd apigateway\ntouch index.js\ntouch schema.js\nnpm init -y\nnpm install express graphql apollo-server-express body-parser graphql-tag apollo-link-http node-fetch\n```\n\nThe schema will created based on the schemas of the other services, so we are going to stitch and expose them in the api gateway.\n\n### schema.js\n\n```javascript\nconst {\n  introspectSchema,\n  makeRemoteExecutableSchema,\n  mergeSchemas,\n} = require(\"graphql-tools\");\nconst { createHttpLink } = require(\"apollo-link-http\");\nconst fetch = require(\"node-fetch\");\n\nconst MoviesUrl = \"http://localhost:3001/api\";\nconst SongsUrl = \"http://localhost:3000/api\";\n\nasync function createServiceSchema(url) {\n  const link = createHttpLink({\n    uri: url,\n    fetch,\n  });\n  const schema = await introspectSchema(link);\n  return makeRemoteExecutableSchema({\n    schema,\n    link,\n  });\n}\n\nasync function createSchemas() {\n  const movieSchema = await createServiceSchema(SongsUrl);\n  const songsSchema = await createServiceSchema(MoviesUrl);\n\n  return mergeSchemas({ schemas: [songsSchema, movieSchema] });\n}\n\nmodule.exports = createSchemas();\n```\n\nAs you can see in the code, the schema is generated by requesting the schemas of both APIs and merging them.\nOne difference is, now we need to request this data before being able to start the apigateway, so the index.js will be slightly different:\n\n```javascript\nconst express = require(\"express\");\nconst { ApolloServer } = require(\"apollo-server-express\");\nconst cors = require(\"cors\");\nconst createSchema = require(\"./schema\");\nconst bodyParser = require(\"body-parser\");\n\nconst app = express();\napp.use(cors());\napp.use(bodyParser.json());\n\ncreateSchema.then((schema) => {\n  const server = new ApolloServer({\n    playground: {\n      endpoint: \"/api\",\n      settings: {\n        \"editor.cursorShape\": \"block\",\n        \"editor.cursorColor\": \"#000\",\n        \"editor.theme\": \"light\",\n      },\n    },\n    schema,\n  });\n\n  server.applyMiddleware({ app, path: \"/api\" });\n\n  app.listen(4000, () => {\n    console.log(\"Graphql listening to 4000...\");\n  });\n});\n```\n\nBefore starting the listener, the schema is requested and merged so we can expose it in our api.\nWe need to run the previous services in order to be able to execute this one. From the root of the project:\n\n```shell\nnode movies/index.js &\nnode songs/index.js &\nnode apigateway/index.js\n```\n\nIf we go to the api gateway playground we can query movies and songs in the same query:\n\n```graphql\n{\n  movies {\n    id\n    title\n    mainSongId\n  }\n\n  songs {\n    id\n    title\n  }\n}\n```\n\nThis was an introduction to schema stitching. In part 2 I will show some more concepts and real case scenarios like extending the services&#x27; schema in the api gateway with custom resolvers, how to optimize by using dataloaders.\nIf you have any questions about graphql schema stitching or about api gateway in general, please add your comment or contact me.\n"},{"title":"Decomplecting shape and optionality","date":"2022-08-28","description":"Separation between the shape of an entity and the optionality of its fields as a way of improving reusability and simplicity.","thumbnail":{"url":"/images/shape-vs-optionality.jpeg","attribution":{"name":"Alfons Morales","url":"https://unsplash.com/@alfonsmc10?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"}},"tags":["schemas","simplicity","information"],"series":{"name":"Schemas","order":1},"filePath":"shape-vs-optionality.mdx","slug":"shape-vs-optionality","markdown":"# Decomplecting shape and optionality\n\n## TLDR;\n\nDefining a schema for a given entity we tend to define what attributes this entity has, what types or invariants\neach of those attributes have and what attributes are required or optional.\n\nIn this post I am exposing that the shape of the data (attributes and their types) need to be discussed separately\nfrom their optionality. If we discuss and define them together we are complecting two concepts that should we separately\nthem the simplicity, clarity and reusability of our system will be greatly improved.\n\n## Introduction\n\nApart from my own experience some of the concepts described in this post are also described by well-known\nengineers like Rich Hickey in [Maybe Not](https://www.youtube.com/watch?v=YR5WdGrpoug) available in my [recommended resources](/recommended-resources)\n\nI will use a Blog Post as the example for the model but again, this applied to any entity we can think of.\n\n## Defining a blog Post\n\n### Defining the shape\n\nWe need to define the data model of a Post that will be used in a blog (any blog).\nAfter some meetings we agree that we need the following fields:\n\n```text\n- ID: uuid that will identiy the post uniquely\n- Title: short text to be shown in links and use as slug\n- Body: text where the post will be written to\n- Tags: list of keywords to find similar posts\n- PublishedAt: datetime when the post was published\n```\n\n### Defining the optionality\n\nOnce we agree on the shape of the Post entity, we start a new discussion:\n\n<Quote>What fields are required and what are optional?</Quote>\n\nIt seems like all are required since we would need to show all of them in the blog. So we move on\nand decide that we will make all columns NOT NULL in the DB.\n\nAfter some time discussing, a junior developer asks:\n\n<Quote>What about drafts?</Quote>\n\nAnother discussion starts about \"What is a draft?\". Should a draft have the same shape as the Post but with \"nullable\"\nfields? what are the required fields in this case?\n\nAfter some time discussing the team decided to have 2 very similar tables one with drafts where columns (except id) are NULLABLE and\nposts where all columns are required.\n\n### Consequences\n\nThere are two entities that somehow are too closely related.\nThis lead to some problems:\n\n```text\n- Entity proliferation. We have all been in projects with too many entities that seem to describe almost the same thing.\n- Change management: New fields in Posts will require new fields in Drafts almost always.\n- Event ambiguity: If we use EDA or similar, are Drafts / Posts events sent to the same topic? are they really different?\n  how can we see the history of a Post if it starts as a Draft?\n- ...\n```\n\n## Rolling back\n\nDefining the shape of the entity is something we cannot avoid (or if we can we should not). Having\na clear description of what an Entity is, and what parts form it is desirable specially if the names\nand types of the fields are defined by the Domain experts.\n\nThe problem started when we discussed about the optionality of the fields. When we started discussion:\n\nWhat fields are required and what are optional?\n\nThe answer to this question should have been another question:\n\n<Quote>When?</Quote>\n(as engineers we should ask `when` more often than we do)\n\n### Optionality per operation\n\nWhat are the operations we can do for/on Posts?\nLet's say that we can do 3 operations with Posts:\n\n```\n- Save the post\n- Publish the post\n- Unpublish the post\n- Delete the post\n```\n\nRules are:\n\n```\n- We cannot edit a post if it is published\n- We cannot delete a post if it is published\n```\n\nOptionality has to do with the operation we want to perform on/with a given Envity.\nFor example, asking:\n\nWhat are the required fields when publishing a post?\n\nThe conclusion would be very similar to the [previous discusion](#defining-the-optionality).\n\nBut if we discussed about what are the required field when saving a Post then the answer will be something around: None.\n\nWe can let users save the posts in whatever state they are (missing tags, missing body, etc)\n\n### Consequences\n\nThere is only one entity that goes through different states until it is published.\nThe benefits are:\n\n```text\n- Entity proliferation: so far under control.\n- Change management: there is only one entity we need to think about.\n- Event unambiguity: all events belong to the same entity so we have one topic and one history line.\n```\n\n## Appendix\n\nIf we changed the rules and we allowed to edit a post while it is published when discussing the optionality during the `save` operation\nwe would need to ask `When` again.\nWhen the post is already published the requirements will be different than when the post is not published yet (or was unpublished).\n"}]},"__N_SSG":true}