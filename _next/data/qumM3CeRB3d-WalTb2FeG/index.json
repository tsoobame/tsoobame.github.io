{"pageProps":{"posts":[{"title":"Resume Driven Development","date":"2024-04-01","description":"A reflection on technology choices in Software Engineering","thumbnail":{"url":"resume-driven-development.jpg","attribution":{"name":"Todd Kent","url":"https://unsplash.com/@churchoftodd?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"}},"tags":["software","design","project","boring"],"filePath":"resume-driven-development.mdx","slug":"resume-driven-development","markdown":"# Resume Driven Development\n\nAs software engineers we want to utilize the most effective tools, patterns and methodologies to fulfill our clients needs.\n\nA common mantra in our industry is `\"Use the right tool for the job\"`.  This idea gained momentum with the microservices hype, leading to a surge in polyglot programming, persistence, and more. This diversification has seen companies adopting a myriad of programming languages, persistence layers, frameworks, and tools in pursuit of the perfect solution, often overlooking the adequacy of existing tools within their stack.\n\nA few years ago, I also fell into the trap of microservices and technology proliferation. Fortunately, the small size of our team and our codebase made the damage reversible and manageable.\n\nThis experience made me pause and take time for introspection. Was I genuinely choosing the right tools for the project or was I unconsciously choosing shiny tools that would make my resume more interesting? \nDespite my best intentions, I couldn't completely disregard the personal desire for learning about them.\n\nThis realization led me to adopt a more judicious and methodical approach to integrating new technologies into our projects\n\n## Adding a new tech to the stack\n\nIntroducing new technology—be it a language, framework, persistence layer, message broker, etc.—requires a learning curve. We love exploring new tools and the novelty of solving use cases and enabling features, needed or not. We tend to underestimate the learning curve of understanding how they fail, when they tend to fail, how the errors are discovered or monitored, and designing how to recover from those errors.\n\nThe real test often comes after deployment, challenging us to reassess the need for that new technology after we've already introduced it.\n\nThus, I urge you to think twice before expanding your tech stack.\n\n## Chosing technology\n\nDuring the introspection period I stumbled into thought-provoking piece titled [Chose boring technology](https://mcfunley.com/choose-boring-technology), which eloquently argued against the rush to adopt new technologies and embrace the boring ones, understanding `boring` as the ones we know and understand well and, therefore, don't give any surprises.\n\nIf your project already has an established tech stack, here are some considerations before introducing new technology:\n\n### Can the existing tech stack address the problem?\n\nDesign the solution with the current stack, without adding any new technology. If the design is feasible, it can be the baseline for the decision. \n\nSome challenges for the design:\n\n- Would it be compliant with the non-functional requirements or SLAs?\n- How complex or maintailable is the implementation?\n\n### Future project features\n\nAssess whether the planned features or project direction could benefit from introducing the tech stack.\nFor example some technologies can act as enablers for future features.\n\n\n### Team expertise\n\nConsider the team's familiarity with the new technology and the implications of acquiring or hiring expertise.\n\n\n## Real-World naive Example\n\nLet's consider a hypothetical project with the following tech stack: \n\n- React in front end\n- Nodejs in the backend (with Express)\n- Postgres as the main database\n\nYour data is structured and fits well in a relational database. Your application is mostly a CRUD application.\n\nLife is good, and things are working as expected until a couple of new features appear in the backlog.\n\n### Feature 1: Integrating unstructured data\n\nYou have a new feature that—for some reason—requires storing unstructured data. When investigating tools, you find Document DBs, one of them being MongoDB.\n\nAlthough it would look great on your—and your team members'—resumes, you think twice, remember a blog post you read some time ago, and decide carefully. That would be the right tool for the job. But, do you really need it?\n\n#### Can the existing tech stack address the problem?\n\nYou create a design with the current stack. After some research, you learn that Postgres supports JSON or JSONB column types that can store unstructured data. Your design includes a JSONB column in the table, and you can query it as you would with a document database.\n\n#### Future project features\n\nAfter scanning through the product backlog, nothing calls your attention that could break your design.\n\n####  Team expertise\n\nThe team has a lot of experience with Postgres and a couple of team members are certified in MongoDB.\n\n#### Decision\n\nKeeping the tech stack and using Postgres' JSONB column type seems the sensible approach.\n\n### Feature 2: Real-time\\* notifications\n\nThe team is to work on a new feature that requires real-time* notifications. Some actions need to be notified to the affected users.\n\nWhen investigating about events, real-time notifications, you find out about message brokers like NATS Server, RabbitMQ, or Kafka. You consider adding them to the tech stack. But—of course—you think twice and follow your process to make the right decision.\n\n#### Can the existing tech stack address the problem?\n\nAfter some research, you find out that PostgreSQL has LISTEN/NOTIFY that supports the Pub/Sub pattern and could be used to subscribe to changes and produce the right notifications. You also read about the outpost pattern that could be used for the feature at hand.\n\nYou create two designs, one using LISTEN/NOTIFY and the other using the outpost pattern.\n\n##### LISTEN/NOTIFY\n\nThis approach could work and your design seems correct.\nOne of the drawbacks is the performance of the database. The more subscribers, the more load on the database. That might impact your queries and the performance of the website.\n\n##### Outpost pattern\n\nAgains, this approach could work and seems elegant and simple.\nOne of the drawbacks is that you would need to add a new table for each subscription type (push notifications, emails, etc).\nFor the feature at hand you only need one subscription but that limitation is something to consider.\n\n#### What other features are down the line?\n\nScanning through the backlog you find multiple stories related to notifications, real-time updates, etc.\nYour choice will need to consider that shift towards real-time event-driven features.\n\n#### How much expertise about that tool do we have in the team?\n\nThe team has a lot of experience with Postgres but not using LISTEN/NOTIFY.\nSome of the team members know well RabbitMQ and/or Kafka.\n\n#### Decision\n\nYou decide that using Postgres alone is not suitable for these features and the ones to come. Adding a message broker is needed and will enable more features in the future. Based on what you learn from researching PostgreSQL, you decide that a combination of the outpost pattern and a message broker is the way to go.\n\nAfter some more research, your team decides to go with RabbitMQ. \n\n## Conclusion\n\nThese examples, while simplified, illustrate the essence of deliberate technology selection. We're naturally drawn to the latest innovations, eager to explore their potential and how they might enhance our projects. But it's crucial to choose mindfully.\n\nAdding a new technology to our stack should come after thoughtful consideration. Even if a tool is perfect for the job, the price of adding it to our stack might outweigh the benefits it brings.\n\n[Personal projects](https://github.com/tonitienda/kadai) offer the perfect playground for experimentation, allowing us to satisfy our appetite for learning without risking the integrity of our professional endeavors. By reserving our exploratory impulses for these spaces, we can approach our work projects with a more focused and pragmatic mindset. What we learn in those personal projects can be applied to our work projects, but only after a thorough evaluation of the potential benefits and drawbacks."},{"title":"Talk, think, code","date":"2024-03-26","description":"Shifting focus in software development: from code-centric to the crucial interplay of conversation, contemplation, and coding","thumbnail":{"url":"talk-think-code.jpg","attribution":{"name":"Paul Bulai","url":"https://unsplash.com/@pcbulai?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"}},"tags":["software","tdd","pair programming"],"filePath":"talk-think-code.mdx","slug":"talk-think-code","markdown":"# Talk, think, code\n\nFor many, software engineering evokes images of relentless coding and complex diagrams. Early in my career, I was no exception, prioritizing coding over thinking and definitely over talking. My aim was simple: deliver features one after another. The approach was straightforward: think only when necessary, find a solution, and then code. Talking was reserved for when there was leftover time.\n\nAs I progressed in my career, it became increasingly clear that effective engineering relies more on discussions than on coding. Excellence in this field is about deeply understanding and breaking down problems through conversation, then carefully considering possible solutions and their trade-offs before choosing the best approach. Only then do we finally code the solution.\n\nThe quality of the software isn't just about the implementation (like clean code), but more importantly, about the quality of the conversations that preceded it and the solutions chosen.\n\nThough I've never formally used Pair Programming or TDD, they were briefly mentioned during a recent interview, and something just clicked. I realized that at their core, these practices highlight the importance of talking and thinking before coding.\n\n## Test Driven Development (TDD)\n\nTDD acts as a powerful tool that encourages developers to pause and think deeply about the problem at hand. It's about considering the problem, coding in the realm of testing while keeping an eye on the solution, and finally, coding it.\n\nTDD stops us from diving straight into coding the solution. It ensures that when we do code, our work is focused, efficient, and truly necessary.\n\n## Pair programming\n\nPair Programming stresses the importance of dialogue and critical thinking before starting to code. It merges different perspectives, informed by unique experiences and understandings of the problem, to develop a shared and refined approach to solving it. This enriches the development process, leading to a clearer understanding, a more detailed problem breakdown, and a more effective solution.\n\nLike TDD, Pair Programming encourages taking a step back to invest more in conversation and thought, slowing down the rush to code.\n\n## Extra: Domain-Driven Design (DDD)\n\nAlthough it was not part of the the interview, DDD also came to my mind. \n\nDomain-Driven Design takes the idea of optimizing conversations and thinking in software development to another level. It's all about crafting a shared language—specific to the project's domain—that everyone, from developers to business stakeholders, uses. This common language ensures that when we talk about the project, there's no gap between what we say and what we code. It's like building a bridge between the problem space and the solution space, making sure that the conversations not only lead to better understanding but also directly influence the implementation in a meaningful way.\n\nDDD emphasizes the importance of deeply understanding the domain you're working in. By defining a ubiquitous language and using it consistently across conversations and in the code, we close the gap that often exists between what needs to be done and how it's executed. This approach streamlines the development process, making it more efficient and ensuring that the solutions are more aligned with the business goals and user needs.\n\nLike TDD and Pair Programming, DDD encourages us to slow down and invest more in the upfront thought process. It shows how powerful and essential clear communication and shared understanding are in delivering high-quality software."},{"title":"Function call cascading considered harmful","date":"2023-05-24","description":"Function call cascading hits the composability, testability and reusability of software components and should be avoided in preference for sequential and explicit function calling.","thumbnail":{"url":"function-call-cascading.jpg","attribution":{"name":"Sarah Doffman","url":"https://unsplash.com/@sarahdoffman?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"}},"tags":["software","anti-patterns","composition"],"filePath":"function-call-cascading.mdx","slug":"function-call-cascading","markdown":"# Function call cascading considered harmful\n\n## Introduction\n\nImproving the reusability and composability of software components is a common goal we all strive for. However, more often than not, we find ourselves falling short without a clear understanding of why. Sometimes, we favor one approach over another without a solid rationale, labeling the differences as mere programming styles.\n\nIn my case, I have a preference for writing code that reads like recipes. I strive to develop functions or components as if they will be utilized by multiple clients, even if that's not the current scenario. This personal style offers benefits such as reusability and testability, but I have also realized that it tends to veer toward overengineering, so it's important to keep that in check.\n\nDuring a recent refactoring process, I encountered a situation where the code followed a different style, and I found it more challenging to reason about, test, and reuse. Yet, I couldn't pinpoint a specific term or a clear, general rationale to explain why I felt that way.\n\nUpon reflection, I discovered the root of the problem, and with the help of Chat GPT, I uncovered the terms associated with those distinct styles.\n\nTo illustrate the problem clearly and make it more apparent, I will provide an example.\n\n## The problem\n\nConsider a web application that processes user registrations. When a new user signs up, various tasks need to be performed, such as validating the user's data, creating a user account, sending a confirmation email, and logging the registration event.\n\n### Function call cascading\n\nOne way of implementing this functionality is using function call cascading.\nWe can visualize it as a sequence:\n\n```mermaid\n\n  sequenceDiagram\n\n  actor Client\n  participant registerUser\n  participant validateUser\n  participant createUser\n  participant sendConfirmationEmail\n  participant logRegistration\n\n  registerUser ->> validateUser: userData\n  validateUser ->> createUser: userData\n  createUser ->> sendConfirmationEmail: userData\n  sendConfirmationEmail ->> logRegistration: userData\n```\n\nAnd the code would look like:\n\n```javascript\nfunction registerUser(userData) {\n  validateUser(userData);\n}\n\nfunction validateUser(userData) {\n  if (userData.username && userData.email && userData.password) {\n    createUser(userData);\n  } else {\n    console.log(\"Invalid user data.\");\n  }\n}\n\nfunction createUser(userData) {\n  // Logic to create a user account\n\n  sendConfirmationEmail(userData);\n}\n\nfunction sendConfirmationEmail(userData) {\n  // Logic to send a confirmation email\n\n  logRegistration(userData);\n}\n\nfunction logRegistration(userData) {\n  // Logic to log the registration event\n\n  console.log(\"User registration completed.\");\n}\n```\n\nThe code will run in sequence, one function after another. If one function fails the next\none will not be executed.\n\nFrom a functional point of view, this works as expected and `registerUser` will do what\nit is intended to.\n\n### Function Orchestration\n\n> Function orchestration: a component organizes and decides the order and need to execute a given set\n> of functions. (I had some troubles naming this style. Let me know if there are better / existing names)\n\nAnother way to implement the same functionality is to use a recipe-like sequence of functions\nto be executed.\n\nAs a sequence it would look like:\n\n```mermaid\n\n  sequenceDiagram\n\n  actor Client\n  participant registerUser\n  participant validateUser\n  participant createUser\n  participant sendConfirmationEmail\n  participant logRegistration\n\n  registerUser ->> validateUser: userData\n  registerUser ->> createUser: userData\n  registerUser ->> sendConfirmationEmail: userData\n  registerUser ->> logRegistration: userData\n```\n\nAnd this is the code:\n\n```javascript\nfunction registerUser(userData) {\n  if (validateUser(userData)) {\n    createUser(userData);\n    sendConfirmationEmail(userData);\n    logRegistration(userData);\n    console.log(\"User registration completed.\");\n  } else {\n    console.log(\"Invalid user data.\");\n  }\n}\n\nfunction validateUser(userData) {\n  return userData.username && userData.email && userData.password;\n}\n\nfunction createUser(userData) {\n  // Logic to create a user account\n}\n\nfunction sendConfirmationEmail(userData) {\n  // Logic to send a confirmation email\n}\n\nfunction logRegistration(userData) {\n  // Logic to log the registration event\n}\n```\n\n## Analysis\n\nAlthough the example is very basic and might be obvious that something is off in the first example,\nI have seen this style in multiple projects I have worked on. Usually hidden in complex logic.\n\nIn the following subsections you can find some of the reasons why the second approach should be preferred.\n\n### Objective coupling\n\nWhen employing function call cascading, all functions become tightly coupled to the objectives of the functions they call down the line.\n\nFor instance, let's consider the `registerUser` function with the following goals:\n\n- Validate user\n- Create user\n- Send confirmation email\n- Log registration\n\nIn this case, the `validateUser` function will **inherit** the same goals as `registerUser`. Similarly, the `createUser` function will inherit all goals except for the validation of the user, and so on.\n\n```mermaid\n\nflowchart LR\n\nsubgraph Functions\n  registerUser\n  validateUser\n  createUser\n  sendConfirmationEmail\n  logRegistration\nend\n\nsubgraph Goals\n  validate-user[validate user]\n  create-user[create user]\n  send-confirmation-email[send confirmation email]\n  log-registration[log registration]\nend\n\nregisterUser --> validateUser\nregisterUser -.- validate-user\nregisterUser -.- create-user\nregisterUser -.- send-confirmation-email\nregisterUser -.- log-registration\n\n\nvalidateUser --> createUser\nvalidateUser -.->validate-user\nvalidateUser -.- create-user\nvalidateUser -.- send-confirmation-email\nvalidateUser -.- log-registration\n\n\ncreateUser --> sendConfirmationEmail\ncreateUser -.->create-user\ncreateUser -.- send-confirmation-email\ncreateUser -.- log-registration\n\n\nsendConfirmationEmail --> logRegistration\nsendConfirmationEmail -.->send-confirmation-email\nsendConfirmationEmail -.- log-registration\n\n\nlogRegistration -.->log-registration\n```\n\n```mermaid\n\nflowchart LR\n\nsubgraph Functions\n  registerUser\n  validateUser\n  createUser\n  sendConfirmationEmail\n  logRegistration\nend\n\nsubgraph Goals\n  validate-user[validate user]\n  create-user[create user]\n  send-confirmation-email[send confirmation email]\n  log-registration[log registration]\nend\n\nregisterUser --> validateUser\nregisterUser --> createUser\nregisterUser --> sendConfirmationEmail\nregisterUser --> logRegistration\n\nregisterUser -.- validate-user\nregisterUser -.- create-user\nregisterUser -.- send-confirmation-email\nregisterUser -.- log-registration\n\n\nvalidateUser -.->validate-user\n\ncreateUser -.->create-user\n\nsendConfirmationEmail -.->send-confirmation-email\n\nlogRegistration -.->log-registration\n```\n\nThis means that I cannot use any of the intermediate functions in a different context than the one aimed at achieving all those goals.\n\nI intentionally use the term **inherit** because the problem is reminiscent of the issues that arise when using inheritance for code reusability.\n\n### Single responsibility\n\nIn line with the issue of objective coupling, every function that calls another function becomes coupled with the responsibilities of the subsequent function. While this coupling may make sense in some cases, such as the `registerUser` function being responsible for all the effects (storing in a database, sending emails, etc.) related to user registration, even if it delegates each effect to a specific function.\n\nOn the other hand, the `validateUser` function should solely be responsible for user validation. However, by utilizing function call cascading, we inadvertently introduce the responsibility of creating a user, sending emails, and so on.\n\n### Testing\n\nWhen conducting integration testing on the `registerUser` function (or any other top-level entry point), it becomes necessary to mock the entire I/O environment to ensure\n\ncorrect behavior. However, it would be preferable to conduct unit tests on the individual composed functions in isolation.\n\nAs a result of the objective and responsibility coupling, when function call cascading is used, we find ourselves obligated to mock the environment for testing each of the composed functions as well.\n\n> It's worth noting that certain languages, like Python with pytest fixtures, provide mechanisms that facilitate the mocking of intermediate functions.\n\n## Conclusion\n\nIn general, to enhance code reusability, testability, readability, and more, it is crucial to be self-conscious when encountering deep function call cascades.\nTake a moment to consider whether the flow you are attempting to describe could be flattened and written as a recipe instead.\nI firmly believe that by keeping a vigilant eye on this aspect, we can improve both our code and our overall development experiences.\n\n### But...\n\nAs most (interesting and fun) things in software, decisions on what is best depends on the context of the decision\nand creating general rules is oversimplistic.\n\nAs a counter example of my own post, there are some patterns that promote the use of function call cascading while\nmaintaining the flexibility of the system.\nFor example when we use [Chain of responsibility](https://refactoring.guru/design-patterns/chain-of-responsibility) like\nin middlewares, each middleware will call the next. But each of them are unaware of what middleware is next since\nthat decision is taken in another component, and therefore the middlewares can be plugged together in any way.\n"},{"title":"Local docker registry mirror","date":"2022-09-05","description":"Why I configured a docker registry in my local machine to improve my development process","thumbnail":{"url":"docker-registry-mirror.jpg","attribution":{"name":"Guillaume Bolduc","url":"https://unsplash.com/@guibolduc"}},"tags":["docker","development","tools"],"filePath":"docker-registry-mirror.mdx","slug":"docker-registry-mirror","markdown":"# Docker Registry Local Mirror\n\n## Why I needed it\n\nI recently started working on some personal projects to do and learn things that I had in my TODO list for a very long time.\nOne the moments when I work on it is during my (long) commuting times when I go to the office or somewhere else by train.\nIn order to get my docker images I need internet connection provided by my phone and it brings a couple of problems:\n\n- I don't have a large amount of GB per month so I am exhausting my available data pretty soon.\n- When I am in tunnels, etc. the connection is dropped.\n\nWhen docker images are new, there is no way to escape it, but I usually use the same docker images for the same projects. Once they\nare downloaded there is no problem but I like keeping the amount of images in my system under control, so I want to be able to do:\n\n```bash\ndocker rmi -f $(docker images -aq)\n```\n\noften.\n\nI thought that if would be cool to have a mirror of the docker registry with the images I use so I can keep running this command and `pull` the images\nfrom my computer.\n\nThat's why I decided to try out and add a docker registri mirror in my machine. And it turned out much easier than I expected.\n\n## How it works\n\n<div style={{ display: \"none\"}}>\n\n```mermaid\nsequenceDiagram\n\nactor me\nparticipant DockerDaemon\nparticipant RegistryMirror\nparticipant DockerRegistry\n\n\nme ->>+ DockerDaemon: docker-compose(up)\nDockerDaemon ->> RegistryMirror: start\nDockerDaemon -->>- me: ok\n\nrect rgb(200,230,250)\n    note right of me: Online / First pull\n\n    me ->>+ DockerDaemon: pull(ubuntu)\n    DockerDaemon ->> RegistryMirror: pull(ubuntu)\n    RegistryMirror ->> DockerRegistry: pull(ubuntu)\n    DockerRegistry -->> RegistryMirror: ubuntu\n    RegistryMirror ->> RegistryMirror: store(ubuntu)\n    RegistryMirror -->> DockerDaemon: ubuntu\n    DockerDaemon -->> me: \nend\n\nrect rgb(200,230,250)\n    note right of me: Offline / Second pull\n\n    me ->>+ DockerDaemon: pull(ubuntu)\n    DockerDaemon ->> RegistryMirror: pull(ubuntu)\n    RegistryMirror -->> DockerDaemon: ubuntu\n    DockerDaemon -->> me: \nend\n\nrect rgb(200,230,250)\n    note right of me: Offline / First pull\n\n    me ->>+ DockerDaemon: pull(alpine)\n    DockerDaemon ->> RegistryMirror: pull(alpine)\n    RegistryMirror -x DockerRegistry: pull(alpine)\n    RegistryMirror --x DockerDaemon: error\n    DockerDaemon --x me: error\nend\n```\n</div>\n\n![docker registry sequence diagram](https://tsoobame.github.io/images/posts/docker-registry-mermaid.png)\n<sub>I did not configure mermaid in my blog yet. I will replace the image with the diagram when it is ready</sub>\n\n## How can you use it\n\nThe code and the explanation on how to use it can be found in the [docker-registry-mirror repository](https://github.com/tonitienda/docker-registry-mirror). \n\n\nHappy (and low data usage) programming!"},{"title":"Decomplecting shape and optionality","date":"2022-08-28","description":"Separation between the shape of an entity and the optionality of its fields as a way of improving reusability and simplicity.","thumbnail":{"url":"shape-vs-optionality.jpg","attribution":{"name":"Alfons Morales","url":"https://unsplash.com/@alfonsmc10?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"}},"tags":["schemas","simplicity","information"],"series":{"name":"Schemas","order":1},"filePath":"shape-vs-optionality.mdx","slug":"shape-vs-optionality","markdown":"# Decomplecting shape and optionality\n\n## TLDR;\n\nDefining a schema for a given entity we tend to define what attributes this entity has, what types or invariants\neach of those attributes have and what attributes are required or optional.\n\nIn this post I am exposing that the shape of the data (attributes and their types) need to be discussed separately\nfrom their optionality. The optionality does not belong to the entity but to the operation being executed.\nIf we discuss and define both concepts together as part of the Entity we are complecting two concepts that should we separate\nthem, the simplicity, clarity and reusability of our system will be greatly improved.\n\n## Introduction\n\nApart from my own experience some of the concepts described in this post are also described by well-known\nengineers like Rich Hickey in [Maybe Not](https://www.youtube.com/watch?v=YR5WdGrpoug) available in my [recommended resources](/recommended-resources)\n\nI will use a Blog Post as the example for the model but the concepts in this post can be applied to any entity and discussion we can think of.\n\n## Defining a blog Post\n\n### Defining the shape\n\nWe need to define the data model of a Post that will be used in a blog (any blog).\nAfter some meetings we agree that we need the following fields:\n\n```text\n- ID: uuid that will identiy the post uniquely\n- Title: short text to be shown in links and use as slug\n- Body: text where the post will be written to\n- Tags: list of keywords to find similar posts\n- PublishedAt: datetime when the post was published\n```\n\n### Defining the optionality\n\nOnce we agree on the shape of the Post entity, we start a new discussion:\n\n<Quote>What fields are required and what are optional?</Quote>\n\nIt seems like all are required since we would need to show all of them in the blog. So we move on\nand decide that we will make all columns NOT NULL in the DB.\n\nAfter some time discussing, a junior developer asks:\n\n<Quote>What about drafts?</Quote>\n\nAnother discussion starts about \"What is a draft?\". Should a draft have the same shape as the Post but with \"nullable\"\nfields? what are the required fields in this case?\n\nAfter some time discussing the team decided to have 2 very similar tables one with drafts where columns (except id) are NULLABLE and\nposts where all columns are required.\n\n### Consequences\n\nThere are two entities that somehow are too closely related.\nThis lead to some problems:\n\n- ⛔ Entity proliferation. We have all been in projects with too many entities that seem to describe almost the same thing.\n- ⛔ Change management: New fields in Posts will require new fields in Drafts almost always.\n- ⛔ Event ambiguity: If we use EDA or similar, are Drafts / Posts events sent to the same topic? are they really different?\n  how can we see the history of a Post if it starts as a Draft?\n- ✅ DB leverage: we can use DB schemas to control whether the data being stored is correct or not.\n\n## Rolling back\n\nDefining the shape of the entity is something we cannot avoid (or if we can we should not). Having\na clear description of what an Entity is, and what parts form it is desirable specially if the names\nand types of the fields are defined by the Domain experts.\n\nThe problem started when we discussed about the optionality of the fields. When we started discussion:\n\nWhat fields are required and what are optional?\n\nThe answer to this question should have been another question:\n\n<Quote>When?</Quote>\n(as engineers we should ask `when` more often than we do)\n\n### Optionality per operation\n\nWhat are the operations we can do for/on Posts?\nLet's say that we can do 3 operations with Posts:\n\n- Save the post\n- Publish the post\n- Unpublish the post\n- Delete the post\n\nRules are:\n\n- We cannot edit a post if it is published\n- We cannot delete a post if it is published\n\nOptionality has to do with the operation we want to perform on/with a given Envity.\nFor example, asking:\n\nWhat are the required fields when publishing a post?\n\nThe conclusion would be very similar to the [previous discusion](#defining-the-optionality).\n\nBut if we discussed about what are the required field when saving a Post then the answer will be something around: None.\n\nWe can let users save the posts in whatever state they are (missing tags, missing body, etc)\n\n### Consequences\n\nThere is only one entity that goes through different states until it is published.\nThe benefits are:\n\n- ✅ Entity proliferation: so far under control.\n- ✅ Change management: there is only one entity we need to think about.\n- ✅ Event unambiguity: all events belong to the same entity so we have one topic and one history line.\n- ⛔ DB leverage: DB Schema cannot be leveraged to control required fields/columns. The control is in the application layer.\n\n## Appendix\n\nIf we changed the rules and we allowed to edit a post while it is published when discussing the optionality during the `save` operation\nwe would need to ask `When` again.\nWhen the post is already published the requirements will be different than when the post is not published yet (or was unpublished).\n"},{"title":"Understanding data loader","date":"2020-07-19","description":"Create a basic dataloader to understand this great library","thumbnail":{"url":"data-loader.jpg","attribution":{"name":"Lars Kienle","url":"https://unsplash.com/@larskienle?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"}},"tags":["javascript"],"filePath":"data-loader.mdx","slug":"data-loader","markdown":"# Understanding the Dataloader\n\nDataloader is one of the packages I find more useful and smart from the ones I have in my toolbox.\n\nI am going to set up a obvious naive example and follow the process to build a simple dataloader to understand its beauty and how useful it is.\n\n## About the project\n\nWe are going to create a view and api over a social network. Our users relations are:\n\n```text\nUser 1 friend of [ 2, 3 ]\nUser 2 friend of [ 1, 3 ]\nUser 3 friend of [ 1, 2, 4 ]\nUser 4 friend of [ 3, 5 ]\nUser 5 friend of [ 4 ]\n```\n\nThe view can show the relation between users and their friends. We can show N levels of their friendship. We are not goint to look much at it in this post.\n\nUsers data can be found here.\n\nThe only dependency will be express.\n\n## Initial Setup\n\n### datasource.js\n\nThe datasource allows us to retrieve one or multiple users by id. Contract is not random, it is already based on the real dataloader so there will be minimal changes over the course of the post. Data is defined in a file within the project. Code is pretty simple:\n\n```javascript\nconst users = require(\"./users.json\");\n\nconst getUsersFromFile = (ids) =>\n  ids.map((id) => users.find((u) => u.id === id));\n\nconst sleep = (ms) => new Promise((resolve) => setTimeout(resolve, ms));\n\nasync function loadMany(ids) {\n  console.log(`GET /users?ids=${ids}`);\n\n  await sleep(100);\n  return getUsersFromFile(ids);\n}\n\nasync function load(id) {\n  const results = await loadMany([id]);\n  return results[0];\n}\n\nmodule.exports = {\n  load,\n  loadMany,\n};\n```\n\nThe only interesting method is loadMany. We will print the requests to the simulated service so we can check the console. There will be a delay to resolve the promise, so we can simulate better and understand why dataloader is so good.\n\nA very important requirement is that data needs to be returned to the caller in the right order and all elements need to be returned (same length of ids and results arrays). This will be clear when we put in place the dataloader.\n\n### resolver.js\n\nResolver will use the datasource received by parameter to load friendship data about users. It can receive the levels of friends we want to get, so it will use a recursive approach to load friends of friends until all levels are fetched.\n\n```javascript\nasync function getFriends(datasource, user, levels) {\n  if (levels == 0) {\n    return { id: user.id, name: user.name };\n  }\n\n  const friends = await datasource.loadMany(user.friends);\n\n  return {\n    ...user,\n    friends: await Promise.all(\n      friends.map((f) => getFriends(datasource, f, levels - 1))\n    ),\n  };\n}\n\nasync function getUserWithFriends(datasource, id, levels = 1) {\n  const user = await datasource.load(id);\n  return getFriends(datasource, user, levels);\n}\n\nmodule.exports = { getUserWithFriends };\n```\n\nIt uses a brute force approach on purpose. The code is simple but far away from being optimal. In one method it looks obvious, but sometimes, when we are building graphql or similar apis, or complex workflows we might be doing exactly this kind of brute force requests.\n\n### view.js\n\nNothing advanced. Just render users friends in a nested way.\n\n```javascript\nfunction render(user) {\n  return `<div style=\"padding-left: 12px;background-color:#def\"> ${user.name} ${\n    user.friends ? user.friends.map((u) => render(u)).join(\"\") : \"\"\n  } </div>`;\n}\n\nmodule.exports = {\n  render,\n};\n```\n\n### server.js\n\n```javascript\nconst express = require(\"express\");\nconst PORT = 3000;\nconst app = express();\n\nconst datasource = require(\"./datasource\");\nconst resolver = require(\"./resolver\");\nconst view = require(\"./view\");\n\napp.get(`/user-with-friends/:id`, async (req, res) => {\n  const id = req.params.id;\n  const levels = req.query.levels || 1;\n\n  const user = await resolver.getUserWithFriends(datasource, id, levels);\n\n  res.send(view.render(user));\n});\n\napp.listen(PORT, () => console.log(`Fakebook listening to ${PORT}`));\n```\n\n## Run\n\n```shell\nnode index.js\n```\n\n## Test 1\n\nWe will render friends of user 1. Only 1 level:\n\n```text\nhttp://localhost:3000/user-with-friends/1\n```\n\nIf we check in our console we will find:\n\n```text\nGET /users?ids=1\nGET /users?ids=2,3\n```\n\nAll good. We requested user 1 and their friends 2 and 3.\n\n## Test 2\n\nLet's try by loading 3 levels:\n\n```text\nhttp://localhost:3000/user-with-friends/1?levels=3\n```\n\nThings are getting interesting here:\n\n```text\nGET /users?ids=1\nGET /users?ids=2,3\nGET /users?ids=1,3\nGET /users?ids=1,2,4\nGET /users?ids=2,3\nGET /users?ids=1,2,4\nGET /users?ids=2,3\nGET /users?ids=1,3\nGET /users?ids=3,5\n```\n\nWe are loading data for users 1,2,3,4,5 but we are doing 9 requests. We are requesting the same users again and again. We could easily improve the situation adding some sort of cache per request.\n\nCache per request\nWe are going to add a cache to the system. It will be empty at the start of each request, so we do not need to worry about expirations. The benefits will be:\n\nDo not request the same resource twice to the remote source during the same request.\nAs side effect, if we try to get the same resource twice during the same request, we will get the same data. So mutations of the resources in between a request will not provide incoherent results.\n\n### cache.js\n\nSimple cache implementation:\n\n```javascript\nfunction make(loadManyFn) {\n  const cache = {};\n\n  async function loadMany(ids) {\n    const notCachedIds = ids.filter((id) => !cache[id]);\n\n    if (notCachedIds.length > 0) {\n      const results = await loadManyFn(notCachedIds);\n      notCachedIds.forEach((id, idx) => (cache[id] = results[idx]));\n    }\n\n    return ids.map((id) => cache[id]);\n  }\n\n  return {\n    load: async (id) => {\n      const results = await loadMany([id]);\n      return results[0];\n    },\n    loadMany,\n  };\n}\n\nmodule.exports = { make };\n```\n\nCache needs a function to retrieve multiple data by id (or in general by a key). It will check the data that is cached and request only the ids that are not found.\n\nImplements the same contract as datasource.\n\n### server.js\n\nLet's add this line to the server:\n\n```javascript\nconst cache = require('./cache')\nAnd replace this line:\n\nconst user = await resolver.getUserWithFriends(datasource, id, levels)\nwith:\n\nconst user = await resolver.getUserWithFriends(cache.make(datasource.loadMany), id, levels)\n```\n\n## Run\n\nLet's run again the server and test the previous request:\n\n```text\nhttp://localhost:3000/user-with-friends/1?levels=3\n```\n\n```text\nGET /users?ids=1\nGET /users?ids=2,3\nGET /users?ids=4\nGET /users?ids=4\nGET /users?ids=5\n```\n\nWe could reduce the number of requests from 9 to 5, which is pretty good. But, what a momentwhat happened here? Why are we requesting id=4 twice?\n\nIf we unnest the request flow based on how nodejs works (and how we implemented our resolver) this is what happened:\n\n```text\n1 - Load user 1 => GET /users?ids=1\n2 - Load friends of 1: [2,3]=> GET /users?ids=2,3\n3.1. Load friends of 2: [1,3] => all cached\n4.1. Load friends of 1 : [2,3] => all cached\n4.2. Load friends of 3 : [1,2,4] => GET /users?ids=4\n3.2. Load friends of 3: [1,2,4] => GET /users?ids=4\n4.3. Load friends of 1: [2,3] => all cached\n4.4. Load friends of 2: [1,3] => all cached\n4.5. Load friends of 4: [3,5] => GET /users?ids=5\nOn 3.1 we had all friends of user 2 cached. So the code was straight to 4.2, than ran in parallel with 3.2. Both were waiting for the same user (4) and therefore made the same requests twice.\n```\n\nSo with our simple cache, we did not reduce the requests to the minimun we wanted.\n\nFor example, if we did:\n\n```javascript\nconst users = await Promise.all(load(1), load(1));\n```\n\nThere would be 2 requests before the cache has data for id=1.\n\nLet's fix this and produce the ideal:\n\n```text\nGET /users?ids=1\nGET /users?ids=2,3\nGET /users?ids=4\nGET /users?ids=5\n```\n\n## Dataloader\n\nUsing nodejs `process.nextTick(...)` we can postpone the execution of a given function to the end of the current event loop cycle. It is useful to run a given function after all variables are initialized for example.\n\nFrom nodejs documentation:\n\n```text\nBy using process.nextTick() we guarantee that apiCall() always runs its callback after the rest of the user's code and before the event loop is allowed to proceed.\n```\n\nUsing it we can accumulate all the keys that are being requested during the same cycle (3.2 and 4.2 in the example above) and request them at the end. In the next cycle we would accumulate again the ones that were depending in the previous ones and so on.\n\nThis simple version of dataloader incorporates also code to accomplish the cache:\n\n```javascript\nfunction make(loadManyFn) {\n  const cache = {};\n  let pending = [];\n  let scheduled = false;\n  function scheduleSearch() {\n    if (pending.length > 0 && !scheduled) {\n      scheduled = true;\n      Promise.resolve().then(() =>\n        process.nextTick(async () => {\n          await runSearch();\n          scheduled = false;\n        })\n      );\n    }\n  }\n\n  async function runSearch() {\n    const pendingCopy = pending.splice(0, pending.length);\n    pending = [];\n\n    if (pendingCopy.length > 0) {\n      const results = await loadManyFn(pendingCopy.map((p) => p.id));\n      pendingCopy.forEach(({ resolve }, idx) => resolve(results[idx]));\n    }\n  }\n\n  async function loadMany(ids) {\n    const notCachedIds = ids.filter((id) => !cache[id]);\n\n    if (notCachedIds.length > 0) {\n      notCachedIds.map((id) => {\n        cache[id] = new Promise((resolve) => {\n          pending.push({ id, resolve });\n        });\n      });\n\n      scheduleSearch();\n    }\n\n    return Promise.all(ids.map((id) => cache[id]));\n  }\n\n  return {\n    load: async (id) => {\n      const results = await loadMany([id]);\n      return results[0];\n    },\n    loadMany,\n  };\n}\n\nmodule.exports = { make };\n```\n\nIgnoring the part of the cache, the important bits are:\n\n### Accumulating requests\n\n```javascript\nnotCachedIds.map((id) => {\n  cache[id] = new Promise((resolve) => {\n    pending.push({ id, resolve });\n  });\n});\n```\n\nWe will add to the list of pending ids the ones that are not cached. We will keep the id and the resolve method, so we can resolve them afterwards with the right value. We cache the promise itself in the hashmap. This would allow us to cache also rejected promises for example. So we do not request over and over the same rejection. It is not used in this implementation, though.\n\n### Scheduling the request\n\n```javascript\nfunction scheduleSearch() {\n  if (pending.length > 0 && !scheduled) {\n    scheduled = true;\n    Promise.resolve().then(() =>\n      process.nextTick(async () => {\n        await runSearch();\n        scheduled = false;\n      })\n    );\n  }\n}\n```\n\nThat is where the magic happens. This function is short but is the most important one: We schedule/delay the request to the end of all the promises declarations.\n\n### Executing the search\n\n```javascript\nasync function runSearch() {\n  const pendingCopy = pending.splice(0, pending.length);\n  pending = [];\n\n  if (pendingCopy.length > 0) {\n    const results = await loadManyFn(pendingCopy.map((p) => p.id));\n    pendingCopy.forEach(({ resolve }, idx) => resolve(results[idx]));\n  }\n}\n```\n\nClone the ids (so they can be accumulated again after the search completes) and call the loadManyFn so we can resolve the promises we had pending. Remember the requirements of loadMany to return the data in the right order and all the elements ? This is where it is needed. We can reference the results by index and resolve the right pending promises.\n\nLet's run it!\n\n## Execution\n\nAgain the same request:\n\n```text\nhttp://localhost:3000/user-with-friends/1?levels=3\n```\n\nThat produces the following output:\n\n```text\nGET /users?ids=1\nGET /users?ids=2,3\nGET /users?ids=4\nGET /users?ids=5\n```\n\nExactly what we wanted.\n\n## Conclusion\n\n- Dataloader is a great package that should be in all developers toolbox. Specially the ones implementing Graphql or similar Apis.\n\n- The resolvers in this example could be optimized but sometimes our requests are on different files at different levels that depend on some conditions. With Dataloader we can keep our file structure and code readability without damaging our performance, both on response time to our client and on number of requests spawn within our mesh.\n\nAre you using Dataloader? Do you know any tool that accomplishes something similar? Do you now any other packages that in your opinion should be in all nodejs devs toolbox?\n"},{"title":"Graphql Schema Stitching","date":"2019-04-26","description":"What Graphql schema stitching is and how it can help us","thumbnail":{"url":"graphql-schema-stitching.jpg","attribution":{"name":"Aneta Pawlik","url":"https://unsplash.com/@anetakpawlik?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"}},"tags":["javascript","graphql","api"],"filePath":"graphql-schema-stitching.mdx","slug":"graphql-schema-stitching","markdown":"# Graphql Schema Stitching\n\nI am going to write a short (?) post about how to create a simple API Gateway that exposes two services using Graphql Stitching. I am assuming some knowledge about graphql and Apollo Server.\nWe will use express, nodejs and apollo for the service and a technique called schema stitching.\nIf you want to learn more about Graphql you can go to the official site.\n\n## Why do we need Api gateways and schema stitching\n\nI will write a whole post about the reasons we had to use Graphql in our services and in our Api Gateway.\nHere I am offering a short explanation:\nIn real world scenarios we are creating independent and autonomous (micro)services. The less data they share, the less they need to call each other and the less coupled they are, the better.\nMany times a service manages entities (or parts of entities) that hold an id about another entity but does not need to know more details. For example an inventory service might manage productID and available units, but does not need to know about the name of the product or about its price.\nInventory service will be able to run all its operations and apply the rules it manages without requesting information to any other service.\nUsers, on the other hand, will need to see this scattered data together in one screen. In order to avoid too many requests from the UI, an API Gateway can offer a single endpoint where UI can request the data needed for a specific functionality/screen in one request, and the Gateway can orchestrate the calls to other services, cache results if needed, etc.\n\n## Let&#x27;s start working\n\nLet&#x27;s create a folder as the root for our project:\n\n```shell\nmkdir graphql-stitching\ncd graphql-stitching\n```\n\n## Creating the songs service\n\nWe are going to create a simple service that offers data about songs.\n\n```shell\nmkdir songs\ncd songs\nnpm init -y\nnpm install express graphql apollo-server-express body-parser\n```\n\nWe are going to create our schema first:\n\n```shell\ntouch schema.js\n```\n\n#### schema.js\n\n```javascript\nconst { makeExecutableSchema } = require(\"graphql-tools\");\nconst gql = require(\"graphql-tag\");\n\nconst songs = [\n  { id: 1, title: \"I will always love you\" },\n  { id: 2, title: \"Lose yourself\" },\n  { id: 3, title: \"Eye of the tiger\" },\n  { id: 4, title: \"Men in Black\" },\n  { id: 5, title: \"The power of love\" },\n  { id: 6, title: \"My Heart will go on\" },\n];\n\nconst typeDefs = gql`\n  type Query {\n    songs: [Song]\n    song(songId: ID!): Song\n  }\n  type Song {\n    id: ID\n    title: String\n  }\n`;\n\nconst resolvers = {\n  Query: {\n    songs: () => {\n      return songs;\n    },\n    song(parent, args, context, info) {\n      return songs.find((song) => song.id === Number(args.songId));\n    },\n  },\n};\n\nmodule.exports = makeExecutableSchema({\n  typeDefs,\n  resolvers,\n});\n```\n\nWe are defining a list of songs.\nThe type Song (id, title) and two queries for getting all songs and one song by id.\n\nLet&#x27;s create the api:\n\n```shell\ntouch index.js\n```\n\n### index.js\n\n```javascript\nconst express = require(\"express\");\nconst { ApolloServer } = require(\"apollo-server-express\");\nconst cors = require(\"cors\");\nconst schema = require(\"./schema\");\nconst bodyParser = require(\"body-parser\");\n\nconst app = express();\napp.use(cors());\napp.use(bodyParser.json());\n\nconst server = new ApolloServer({\n  playground: {\n    endpoint: \"/api\",\n    settings: {\n      \"editor.cursorShape\": \"block\",\n      \"editor.cursorColor\": \"#000\",\n      \"editor.theme\": \"light\",\n    },\n  },\n  schema,\n});\n\nserver.applyMiddleware({ app, path: \"/api\" });\n\napp.listen(3000, () => {\n  console.log(\"Song services listening to 3000...\");\n});\n```\n\nWe create a simple express service using apollo engine to expose both the api and the playground to tests our api.\n\n```shell\nnode index.js\n```\n\nand open the <a href=\"http://localhost:3000\">songs api</a>\nYou will see the playground, so you can run the first query:\n\n```graphql\n{\n  songs {\n    id\n    title\n  }\n}\n```\n\nyou should be able to see the results.\n\n## Creating the movies service\n\nWe are going to follow the same process. From the root of our project:\n\n```shell\nmkdir movies\ncd movies\ntouch index.js\ntouch schema.js\nnpm init -y\nnpm install express graphql apollo-server-express body-parser graphql-tag\n```\n\nindex.js will be similar to the previous one. Only the port number needs to be different\n\n```javascript\nconst express = require(\"express\");\nconst { ApolloServer } = require(\"apollo-server-express\");\nconst cors = require(\"cors\");\nconst schema = require(\"./schema\");\nconst bodyParser = require(\"body-parser\");\n\nconst app = express();\napp.use(cors());\napp.use(bodyParser.json());\n\nconst server = new ApolloServer({\n  playground: {\n    endpoint: \"/api\",\n    settings: {\n      \"editor.cursorShape\": \"block\",\n      \"editor.cursorColor\": \"#000\",\n      \"editor.theme\": \"light\",\n    },\n  },\n  schema,\n});\n\nserver.applyMiddleware({ app, path: \"/api\" });\n\napp.listen(3001, () => {\n  console.log(\"Movie services listening to 3001...\");\n});\n```\n\nSchema will be very similar:\n\n```javascript\nconst { makeExecutableSchema } = require(\"graphql-tools\");\nconst gql = require(\"graphql-tag\");\n\nconst movies = [\n  { id: 1, title: \"The Bodyguard\", mainSongId: 1 },\n  { id: 2, title: \"8 Mile\", mainSongId: 2 },\n  { id: 3, title: \"Rocky III\", mainSongId: 3 },\n  { id: 4, title: \"Men in Black\", mainSongId: 4 },\n  { id: 5, title: \"Back to the Future\", mainSongId: 5 },\n  { id: 6, title: \"Titanic\", mainSongId: 6 },\n];\n\nconst typeDefs = gql`\n  type Query {\n    movies: [Movie]\n    movie(movieId: ID!): Movie\n  }\n  type Movie {\n    id: ID!\n    title: String!\n    mainSongId: ID!\n  }\n`;\n\nconst resolvers = {\n  Query: {\n    movies: () => {\n      return movies;\n    },\n    movie(parent, args, context, info) {\n      return movies.find((movie) => movie.id === Number(args.movieId));\n    },\n  },\n};\n\nmodule.exports = makeExecutableSchema({\n  typeDefs,\n  resolvers,\n});\n```\n\nThe difference is that movie has a reference to songs. Specifically mainSongId. Since both services are isolated and are autonomous, movie service does not know where songs service is, or what data a songs holds. Only knows that a movie has a main song and it holds its ID.\n\nIf we run the project in the same way\n\n```shell\nnode index.js\n```\n\nwe can see the <a href=\"http://localhost:3001\">playground</a> and run our test queries.\n\n## Let&#x27;s start the interesting part. Our Api gateway\n\nWe are going to create the same files. From project root:\n\n```shell\nmkdir apigateway\ncd apigateway\ntouch index.js\ntouch schema.js\nnpm init -y\nnpm install express graphql apollo-server-express body-parser graphql-tag apollo-link-http node-fetch\n```\n\nThe schema will created based on the schemas of the other services, so we are going to stitch and expose them in the api gateway.\n\n### schema.js\n\n```javascript\nconst {\n  introspectSchema,\n  makeRemoteExecutableSchema,\n  mergeSchemas,\n} = require(\"graphql-tools\");\nconst { createHttpLink } = require(\"apollo-link-http\");\nconst fetch = require(\"node-fetch\");\n\nconst MoviesUrl = \"http://localhost:3001/api\";\nconst SongsUrl = \"http://localhost:3000/api\";\n\nasync function createServiceSchema(url) {\n  const link = createHttpLink({\n    uri: url,\n    fetch,\n  });\n  const schema = await introspectSchema(link);\n  return makeRemoteExecutableSchema({\n    schema,\n    link,\n  });\n}\n\nasync function createSchemas() {\n  const movieSchema = await createServiceSchema(SongsUrl);\n  const songsSchema = await createServiceSchema(MoviesUrl);\n\n  return mergeSchemas({ schemas: [songsSchema, movieSchema] });\n}\n\nmodule.exports = createSchemas();\n```\n\nAs you can see in the code, the schema is generated by requesting the schemas of both APIs and merging them.\nOne difference is, now we need to request this data before being able to start the apigateway, so the index.js will be slightly different:\n\n```javascript\nconst express = require(\"express\");\nconst { ApolloServer } = require(\"apollo-server-express\");\nconst cors = require(\"cors\");\nconst createSchema = require(\"./schema\");\nconst bodyParser = require(\"body-parser\");\n\nconst app = express();\napp.use(cors());\napp.use(bodyParser.json());\n\ncreateSchema.then((schema) => {\n  const server = new ApolloServer({\n    playground: {\n      endpoint: \"/api\",\n      settings: {\n        \"editor.cursorShape\": \"block\",\n        \"editor.cursorColor\": \"#000\",\n        \"editor.theme\": \"light\",\n      },\n    },\n    schema,\n  });\n\n  server.applyMiddleware({ app, path: \"/api\" });\n\n  app.listen(4000, () => {\n    console.log(\"Graphql listening to 4000...\");\n  });\n});\n```\n\nBefore starting the listener, the schema is requested and merged so we can expose it in our api.\nWe need to run the previous services in order to be able to execute this one. From the root of the project:\n\n```shell\nnode movies/index.js &\nnode songs/index.js &\nnode apigateway/index.js\n```\n\nIf we go to the api gateway playground we can query movies and songs in the same query:\n\n```graphql\n{\n  movies {\n    id\n    title\n    mainSongId\n  }\n\n  songs {\n    id\n    title\n  }\n}\n```\n\nThis was an introduction to schema stitching. In part 2 I will show some more concepts and real case scenarios like extending the services&#x27; schema in the api gateway with custom resolvers, how to optimize by using dataloaders.\nIf you have any questions about graphql schema stitching or about api gateway in general, please add your comment or contact me.\n"}]},"__N_SSG":true}